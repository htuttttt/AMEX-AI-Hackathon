{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "SAMPLE_SUBMISSION_ROWS = os.environ.get(\"SAMPLE_SUBMISSION_ROWS\")\n",
    "TRAINING_DATA = os.environ.get(\"TRAINING_DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ind_recommended</th>\n",
       "      <th>activation</th>\n",
       "      <th>customer_digital_activity_04</th>\n",
       "      <th>customer_spend_01</th>\n",
       "      <th>customer_industry_spend_01</th>\n",
       "      <th>customer_industry_spend_02</th>\n",
       "      <th>customer_industry_spend_03</th>\n",
       "      <th>customer_industry_spend_04</th>\n",
       "      <th>customer_industry_spend_05</th>\n",
       "      <th>customer_spend_02</th>\n",
       "      <th>...</th>\n",
       "      <th>merchant_spend_09</th>\n",
       "      <th>merchant_profile_03</th>\n",
       "      <th>customer_digital_activity_01</th>\n",
       "      <th>merchant_spend_10</th>\n",
       "      <th>customer_profile_03</th>\n",
       "      <th>customer_digital_activity_02</th>\n",
       "      <th>customer_profile_04</th>\n",
       "      <th>distance_05</th>\n",
       "      <th>customer</th>\n",
       "      <th>merchant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107.215862</td>\n",
       "      <td>26.686594</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3682.75</td>\n",
       "      <td>138.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>49466.0</td>\n",
       "      <td>65923.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.180</td>\n",
       "      <td>58.434969</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>86.0</td>\n",
       "      <td>15.856826</td>\n",
       "      <td>168972</td>\n",
       "      <td>152285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.552000</td>\n",
       "      <td>50.928261</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1171.35</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3638.0</td>\n",
       "      <td>7801.0</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>28.465</td>\n",
       "      <td>5.392089</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>125.0</td>\n",
       "      <td>6.998555</td>\n",
       "      <td>212404</td>\n",
       "      <td>39032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.623103</td>\n",
       "      <td>48.837872</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2295.38</td>\n",
       "      <td>47.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3912.0</td>\n",
       "      <td>12868.0</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>421.500</td>\n",
       "      <td>33.780445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.753009</td>\n",
       "      <td>225178</td>\n",
       "      <td>7439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112.277391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28919.0</td>\n",
       "      <td>23553.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>50.000</td>\n",
       "      <td>37.340085</td>\n",
       "      <td>28.666667</td>\n",
       "      <td>134.0</td>\n",
       "      <td>9.000063</td>\n",
       "      <td>183948</td>\n",
       "      <td>485069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>448.427273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1086.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>69.509</td>\n",
       "      <td>77.794164</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1.767939</td>\n",
       "      <td>210107</td>\n",
       "      <td>536004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ind_recommended  activation  customer_digital_activity_04  \\\n",
       "0                0           0                           NaN   \n",
       "1                0           0                           NaN   \n",
       "2                0           0                           NaN   \n",
       "3                0           0                           NaN   \n",
       "4                1           0                           NaN   \n",
       "\n",
       "   customer_spend_01  customer_industry_spend_01  customer_industry_spend_02  \\\n",
       "0         107.215862                   26.686594                        74.0   \n",
       "1          35.552000                   50.928261                         3.0   \n",
       "2          31.623103                   48.837872                        19.0   \n",
       "3         112.277391                         NaN                         NaN   \n",
       "4         448.427273                         NaN                         NaN   \n",
       "\n",
       "   customer_industry_spend_03  customer_industry_spend_04  \\\n",
       "0                     3682.75                       138.0   \n",
       "1                     1171.35                        23.0   \n",
       "2                     2295.38                        47.0   \n",
       "3                         NaN                         NaN   \n",
       "4                         NaN                         NaN   \n",
       "\n",
       "   customer_industry_spend_05  customer_spend_02  ...  merchant_spend_09  \\\n",
       "0                       111.0               14.0  ...            49466.0   \n",
       "1                        17.0                2.0  ...             3638.0   \n",
       "2                        42.0               11.0  ...             3912.0   \n",
       "3                         NaN               16.0  ...            28919.0   \n",
       "4                         NaN                5.0  ...             1086.0   \n",
       "\n",
       "   merchant_profile_03  customer_digital_activity_01  merchant_spend_10  \\\n",
       "0              65923.0                      0.000000             29.180   \n",
       "1               7801.0                      0.419355             28.465   \n",
       "2              12868.0                      0.836364            421.500   \n",
       "3              23553.0                      0.952381             50.000   \n",
       "4                308.0                      0.754386             69.509   \n",
       "\n",
       "   customer_profile_03  customer_digital_activity_02  customer_profile_04  \\\n",
       "0            58.434969                     32.500000                 86.0   \n",
       "1             5.392089                      7.000000                125.0   \n",
       "2            33.780445                      0.000000                180.0   \n",
       "3            37.340085                     28.666667                134.0   \n",
       "4            77.794164                     15.000000                114.0   \n",
       "\n",
       "   distance_05  customer  merchant  \n",
       "0    15.856826    168972    152285  \n",
       "1     6.998555    212404     39032  \n",
       "2     1.753009    225178      7439  \n",
       "3     9.000063    183948    485069  \n",
       "4     1.767939    210107    536004  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain training data \n",
    "\n",
    "df = pd.read_csv(TRAINING_DATA)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12229978"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix between features\n",
    "\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Select the features with the greatest correlation to ind_recommended\"\"\"\n",
    "\n",
    "# X_without_ind_reco = df.drop(columns=[\"ind_recommended\", \"activation\"])\n",
    "# y_ind_reco = df[\"ind_recommended\"]\n",
    "\n",
    "# # Calculate correlation coefficients between features and target variable\n",
    "# correlations = X_without_ind_reco.apply(lambda x: x.corr(y_ind_reco))\n",
    "\n",
    "# # Sort features based on absolute correlation values\n",
    "# sorted_features = correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "# # Select features with highest correlation\n",
    "# top_features = sorted_features[:50]  # Select top k features, adjust k as needed\n",
    "\n",
    "# # Extract top features from original dataframe\n",
    "# df_ind_reco_features = X_without_ind_reco[top_features.index]\n",
    "\n",
    "# df_ind_reco_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Select the features with the greatest correlation to activation\"\"\"\n",
    "\n",
    "# X_without_activation = df.drop(columns=[\"activation\"])\n",
    "# y_activation = df[\"activation\"]\n",
    "\n",
    "# # Calculate correlation coefficients between features and target variable\n",
    "# correlations = X_without_activation.apply(lambda x: x.corr(y_activation))\n",
    "\n",
    "# # Sort features based on absolute correlation values\n",
    "# sorted_features = correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "# # Select features with highest correlation\n",
    "# top_features = sorted_features[:50]  # Select top k features, adjust k as needed\n",
    "\n",
    "# # Extract top features from original dataframe\n",
    "# df_activation_features = X_without_activation[top_features.index]\n",
    "\n",
    "# df_activation_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind_reco_features = df_ind_reco_features.columns.to_list()\n",
    "\n",
    "# activation_features = df_activation_features.columns.to_list()\n",
    "\n",
    "# print(ind_reco_features)\n",
    "# print()\n",
    "# print(activation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualise the correlation matrix\n",
    "\n",
    "# plt.figure(figsize=(48, 36))\n",
    "# sb.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "# plt.title(\"Correlation Matrix\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Determine which features have the last null/NaN values\n",
    "\n",
    "# # Count the number of null values in each column\n",
    "# null_counts = df.isnull().sum()\n",
    "\n",
    "# # Sort the columns based on the number of null values in ascending order\n",
    "# sorted_columns = null_counts.sort_values()\n",
    "\n",
    "# # Print the columns with the least null values\n",
    "# print(\"Columns with the least null values:\")\n",
    "# sorted_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter the sorted columns to include only those with no null values\n",
    "# no_null_columns = sorted_columns[sorted_columns == 0]\n",
    "\n",
    "# # Print the columns with no null values\n",
    "# print(\"Columns with no null values:\")\n",
    "# print(no_null_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Remove features with high correlation \"\"\"\n",
    "\n",
    "# CORRELATION_THRESHOLD = 0.8\n",
    "\n",
    "# # Identify highly correlated features\n",
    "# highly_correlated_features = np.where(correlation_matrix > CORRELATION_THRESHOLD)\n",
    "\n",
    "# features_to_remove = set()\n",
    "# for feature1, feature2 in zip(*highly_correlated_features):\n",
    "#     if feature1 != feature2:\n",
    "#         if feature1 not in features_to_remove and feature2 not in features_to_remove:\n",
    "#             features_to_remove.add(feature2)\n",
    "\n",
    "# # Remove the highly correlated features\n",
    "# selected_features = [\n",
    "#     feature for i, feature in enumerate(df.columns) if i not in features_to_remove\n",
    "# ]\n",
    "# # selected_df = df[selected_features]\n",
    "\n",
    "# # print(\"Selected Features:\")\n",
    "# # selected_df.head()\n",
    "\n",
    "# ind_reco_features_selected = [f for f in ind_reco_features if f in selected_features]\n",
    "\n",
    "# print(f\"ind_reco_features: {len(ind_reco_features_selected)}\")\n",
    "\n",
    "# activation_features_selected = [f for f in activation_features if f in selected_features]\n",
    "\n",
    "# print(f\"activation_features: {len(activation_features_selected)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Create a selected_df with the union of the 3 feature groups so that imputation is uniform\"\"\"\n",
    "\n",
    "# core_features = [\"customer\", \"merchant\", \"ind_recommended\", \"activation\"]\n",
    "\n",
    "# union_features = list(set().union(ind_reco_features_selected, activation_features_selected, core_features))\n",
    "\n",
    "# selected_df = df[union_features]\n",
    "\n",
    "# len(union_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# selected_df contains the truncated dataframe after highly-correlated features have been removed.\n",
    "# Now remove columns with over the specified threshold count of null values.\n",
    "# \"\"\"\n",
    "\n",
    "# # Count the null values in each column\n",
    "# null_counts = selected_df.isnull().sum()\n",
    "\n",
    "# NULL_COUNT_THRESHOLD = 3_000_000\n",
    "\n",
    "# # Filter the DataFrame to preserve only columns with less than `threshold` null values\n",
    "# filtered_columns = null_counts[null_counts <= NULL_COUNT_THRESHOLD].index\n",
    "# filtered_df = df[filtered_columns]\n",
    "\n",
    "# print(\"Features in filtered DataFrame:\")\n",
    "# features = filtered_df.columns.to_list()\n",
    "# print(features)\n",
    "# len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add features that are missing from the Customer Industry Spend and Customer Merchant categories\n",
    "\n",
    "# filtered_df[\"customer_merchant_01\"] = df[\"customer_merchant_01\"].values\n",
    "# filtered_df[\"customer_merchant_02\"] = df[\"customer_merchant_02\"].values\n",
    "\n",
    "# filtered_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df[\"customer_industry_spend_04\"] = df[\"customer_industry_spend_04\"].values\n",
    "# filtered_df[\"customer_industry_spend_05\"] = df[\"customer_industry_spend_05\"].values\n",
    "\n",
    "# filtered_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"ind_recommended\",\n",
    "    \"activation\",\n",
    "    \"customer_spend_01\",\n",
    "    \"customer_industry_spend_01\",\n",
    "    \"customer_industry_spend_03\",\n",
    "    \"customer_spend_02\",\n",
    "    \"customer_spend_04\",\n",
    "    \"customer_spend_05\",\n",
    "    \"customer_spend_06\",\n",
    "    \"merchant_spend_06\",\n",
    "    \"merchant_spend_07\",\n",
    "    \"merchant_profile_01\",\n",
    "    \"customer_merchant_03\",\n",
    "    \"customer_digital_activity_05\",\n",
    "    \"customer_spend_15\",\n",
    "    \"customer_digital_activity_16\",\n",
    "    \"customer_spend_17\",\n",
    "    \"customer_digital_activity_17\",\n",
    "    \"customer_digital_activity_03\",\n",
    "    \"distance_01\",\n",
    "    \"customer_digital_activity_20\",\n",
    "    \"distance_02\",\n",
    "    \"distance_03\",\n",
    "    \"customer_spend_18\",\n",
    "    \"customer_spend_19\",\n",
    "    \"customer_digital_activity_21\",\n",
    "    \"customer_digital_activity_22\",\n",
    "    \"distance_04\",\n",
    "    \"merchant_profile_02\",\n",
    "    \"merchant_spend_09\",\n",
    "    \"merchant_profile_03\",\n",
    "    \"customer_digital_activity_01\",\n",
    "    \"merchant_spend_10\",\n",
    "    \"customer_profile_03\",\n",
    "    \"customer_digital_activity_02\",\n",
    "    \"customer_profile_04\",\n",
    "    \"distance_05\",\n",
    "    \"customer\",\n",
    "    \"merchant\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute null values with the mean of each column\n",
    "\n",
    "df_imputed = filtered_df.fillna(filtered_df.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that none of the remaining features have any NULL/NaN values left after interpolation\n",
    "\n",
    "imputed_null_counts = df_imputed.isnull().sum()\n",
    "\n",
    "# Filter the columns to include only those with null values\n",
    "imputed_null_columns = imputed_null_counts[imputed_null_counts > 0]\n",
    "\n",
    "imputed_null_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Create the 2 dataframes\"\"\"\n",
    "\n",
    "# filtered_ind_reco_features = [f for f in features if f in ind_reco_features_selected]\n",
    "\n",
    "# filtered_activation_features = [\n",
    "#     f for f in features if f in activation_features_selected\n",
    "# ]\n",
    "\n",
    "# for core in core_features:\n",
    "#     if core not in filtered_ind_reco_features:\n",
    "#         filtered_ind_reco_features.append(core)\n",
    "#     if core not in filtered_activation_features:\n",
    "#         filtered_activation_features.append(core)\n",
    "\n",
    "# print(f\"filtered ind reco features: {len(filtered_ind_reco_features)}\")\n",
    "# print(f\"filtered activation features: {len(filtered_activation_features)}\")\n",
    "\n",
    "# df_ind_reco = df[filtered_ind_reco_features]  \n",
    "# df_activation = df[filtered_activation_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Prepare X and y for ind_reco\"\"\"\n",
    "\n",
    "# X_reco = df_ind_reco.drop(\n",
    "#     columns=[\"activation\", \"customer\", \"merchant\", \"ind_recommended\"]\n",
    "# )  # drop customer and merchant too as those are not features\n",
    "# y_reco = df_ind_reco[\"ind_recommended\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Prepare X and y for activation\"\"\"\n",
    "\n",
    "# X = df_activation.drop(\n",
    "#     columns=[\"activation\", \"customer\", \"merchant\"]\n",
    "# )  # drop customer and merchant too as those are not features\n",
    "# y = df_activation[\"activation\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 5 folds for K-Fold Cross Validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = df_imputed.drop(\n",
    "    columns=[\"activation\", \"customer\", \"merchant\"]\n",
    ")  # drop customer and merchant too as those are not features\n",
    "y = df_imputed[\"activation\"]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset to predict ind_recommended\n",
    "\n",
    "X_reco = df_imputed.drop(\n",
    "    columns=[\"activation\", \"customer\", \"merchant\", \"ind_recommended\"]\n",
    ")  # drop customer and merchant too as those are not features\n",
    "y_reco = df_imputed[\"ind_recommended\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Randomly sample 5_000_000 indices for each k-fold and save them to ensure consistency across models\n",
    "\n",
    "# # MAX_SAMPLES = 100_000\n",
    "\n",
    "# K_FOLD_INDICES = []\n",
    "\n",
    "# for train_index, test_index in kf.split(X, y):\n",
    "#     # Randomly sample indices for both training and testing\n",
    "#     # train_index = np.random.choice(\n",
    "#     #     train_index, size=min(len(train_index), MAX_SAMPLES), replace=False\n",
    "#     # )\n",
    "#     # test_index = np.random.choice(\n",
    "#     #     test_index, size=min(len(test_index), MAX_SAMPLES), replace=False\n",
    "#     # )\n",
    "\n",
    "#     X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#     y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "#     K_FOLD_INDICES.append([X_train, X_test, y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Randomly sample 5_000_000 indices for each k-fold and save them to ensure consistency across models\n",
    "\n",
    "# # MAX_SAMPLES = 100_000\n",
    "\n",
    "# K_FOLD_INDICES_RECO = []\n",
    "\n",
    "# for train_index, test_index in kf.split(X_reco, y_reco):\n",
    "#     # Randomly sample indices for both training and testing\n",
    "#     # train_index = np.random.choice(\n",
    "#     #     train_index, size=min(len(train_index), MAX_SAMPLES), replace=False\n",
    "#     # )\n",
    "#     # test_index = np.random.choice(\n",
    "#     #     test_index, size=min(len(test_index), MAX_SAMPLES), replace=False\n",
    "#     # )\n",
    "\n",
    "#     X_reco_train, X_reco_test = X_reco.iloc[train_index], X_reco.iloc[test_index]\n",
    "#     y_reco_train, y_reco_test = y_reco.iloc[train_index], y_reco.iloc[test_index]\n",
    "\n",
    "#     K_FOLD_INDICES_RECO.append([X_reco_train, X_reco_test, y_reco_train, y_reco_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start of Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Random Forest Classifier\n",
    "# Perform Hyperparameter tuning on the RFC with Randomized Search & K - Fold\n",
    "# \"\"\"\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# # Define the parameter distribution\n",
    "# param_dist = {\n",
    "#     \"n_estimators\": [50, 100, 200, 300],\n",
    "#     \"max_depth\": [None, 10, 20, 30],\n",
    "#     \"min_samples_split\": [2, 5, 10],\n",
    "#     \"min_samples_leaf\": [1, 2, 4],\n",
    "#     \"bootstrap\": [True, False],\n",
    "# }\n",
    "\n",
    "# # Instantiate the Random Forest classifier\n",
    "# rf = RandomForestClassifier()\n",
    "\n",
    "# # Perform Randomized Search with k-fold cross-validation\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     rf, param_dist, n_iter=10, cv=kf, scoring=\"accuracy\", random_state=42\n",
    "# )\n",
    "# random_search.fit(X, y)\n",
    "\n",
    "# # Get the best hyperparameters\n",
    "# best_params = random_search.best_params_\n",
    "# print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# # Train the final model with the best hyperparameters\n",
    "# model = RandomForestClassifier(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Random Forest Classifier\n",
    "# Based on the selected features and whether a merchant was recommended to a customer, use a Random Forest Classifier to predict\n",
    "# whether a customer will activate.\n",
    "# \"\"\"\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# scores = []\n",
    "# for X_train, X_test, y_train, y_test in K_FOLD_INDICES:\n",
    "\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "\n",
    "#     score = accuracy_score(y_test, y_pred)\n",
    "#     scores.append(score)\n",
    "\n",
    "# # Display cross-validation scores\n",
    "# print(\"Cross-validation scores:\", scores)\n",
    "\n",
    "# # Compute mean cross-validation score\n",
    "# mean_score = sum(scores) / len(scores)\n",
    "# print(\"Mean cross-validation score:\", mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform k-fold cross-validation\n",
    "# # Specify the number of folds (k) in the cv parameter\n",
    "# scores = cross_val_score(clf, X, y, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "# print(\"Cross-validation scores:\", scores)\n",
    "\n",
    "# print(\"Mean accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the clf model since cross_val_score does not return the fitted cloned models.\n",
    "# \"\"\"Logistic Regression\n",
    "# Based on the selected features and whether a merchant was recommended to a customer, use a Logistic Regression Classifier to predict\n",
    "# whether a customer will activate.\n",
    "# \"\"\"\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Instantiate Logistic Regression model\n",
    "# clf = LogisticRegression(max_iter=1000)  # Increase max_iter if needed\n",
    "\n",
    "# # Split data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=58\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split data into train and test sets\n",
    "# X_reco_train, X_reco_test, y_reco_train, y_reco_test = train_test_split(\n",
    "#     X_reco, y_reco, test_size=0.2, random_state=58\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Train Random Forest model to predict ind_recommended\"\"\"\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Instantiate Random Forest Classifier\n",
    "# clf_reco = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# # Fit model\n",
    "# clf_reco.fit(X_reco_train, y_reco_train)\n",
    "\n",
    "# # Make predictions\n",
    "# y_reco_pred = clf_reco.predict(X_reco_test)\n",
    "\n",
    "# # Evaluate model\n",
    "# accuracy = accuracy_score(y_reco_test, y_reco_pred)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Train Complement Naive Bayes Classifier to predict ind_recommended\"\"\"\n",
    "\n",
    "# from sklearn.naive_bayes import ComplementNB\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Instantiate Gaussian Naive Bayes Classifier\n",
    "# clf_reco = ComplementNB()\n",
    "\n",
    "# # Fit model\n",
    "# clf_reco.fit(X_reco_train, y_reco_train)\n",
    "\n",
    "# # Make predictions\n",
    "# y_reco_pred = clf_reco.predict(X_reco_test)\n",
    "\n",
    "# # Evaluate model\n",
    "# accuracy = accuracy_score(y_reco_test, y_reco_pred)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"XGBoost to predict ind_recommended.\"\"\"\n",
    "\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # XGB_BEST_PARAMS = {\"learning_rate\": 0.1, \"max_depth\": 5, \"n_estimators\": 300}\n",
    "\n",
    "# # Instantiate XGBoost Classifier\n",
    "# clf_reco = XGBClassifier()\n",
    "\n",
    "# scores_reco = []\n",
    "# for X_reco_train, X_reco_test, y_reco_train, y_reco_test in K_FOLD_INDICES_RECO:\n",
    "\n",
    "#     clf_reco.fit(X_reco_train, y_reco_train)\n",
    "#     y_reco_pred = clf_reco.predict(X_reco_test)\n",
    "\n",
    "#     score_reco = accuracy_score(y_reco_test, y_reco_pred)\n",
    "#     scores_reco.append(score_reco)\n",
    "\n",
    "# # Display cross-validation scores\n",
    "# print(\"Cross-validation scores:\", scores_reco)\n",
    "\n",
    "# # Compute mean cross-validation score\n",
    "# mean_score = sum(scores_reco) / len(scores_reco)\n",
    "# print(\"Mean cross-validation score:\", mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the random forest classifier model to joblib\n",
    "\n",
    "# from joblib import dump\n",
    "\n",
    "# dump(clf_reco, \"XGBoost/new_formula/reco.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classifier model\n",
    "\n",
    "from joblib import load\n",
    "\n",
    "clf_reco = load(\"XGBoost/new_formula/reco.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"XGBoost to predict activation.\"\"\"\n",
    "\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # # Define hyperparameter grid\n",
    "# # param_grid = {\n",
    "# #     \"max_depth\": [3, 5, 10],\n",
    "# #     \"learning_rate\": [0.1, 0.01, 0.2],\n",
    "# #     \"n_estimators\": [100, 200, 300],\n",
    "# # }\n",
    "\n",
    "# # Instantiate XGBoost Classifier\n",
    "# # xgb = XGBClassifier()\n",
    "\n",
    "# # # Perform grid search\n",
    "# # grid_search = GridSearchCV(\n",
    "# #     estimator=xgb, param_grid=param_grid, scoring=\"accuracy\"\n",
    "# # )\n",
    "# # grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # # Get best hyperparameters\n",
    "# # best_params = grid_search.best_params_\n",
    "# # print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# # # Evaluate best model\n",
    "# # clf = grid_search.best_estimator_\n",
    "# # y_pred = clf.predict(X_test)\n",
    "# # accuracy = accuracy_score(y_test, y_pred)\n",
    "# # print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# # XGB_BEST_PARAMS = {\"learning_rate\": 0.1, \"max_depth\": 5, \"n_estimators\": 300}\n",
    "\n",
    "# # Instantiate XGBoost Classifier\n",
    "# clf = XGBClassifier()\n",
    "\n",
    "# scores = []\n",
    "# for X_train, X_test, y_train, y_test in K_FOLD_INDICES:\n",
    "\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     y_pred = clf.predict(X_test)\n",
    "\n",
    "#     score = accuracy_score(y_test, y_pred)\n",
    "#     scores.append(score)\n",
    "\n",
    "# # Display cross-validation scores\n",
    "# print(\"Cross-validation scores:\", scores)\n",
    "\n",
    "# # Compute mean cross-validation score\n",
    "# mean_score = sum(scores) / len(scores)\n",
    "# print(\"Mean cross-validation score:\", mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Gaussian Naive Bayes to predict activation.\"\"\"\n",
    "\n",
    "# # Instantiate Gaussian Naive Bayes Classifier\n",
    "# clf = ComplementNB()\n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the classifier model to joblib\n",
    "\n",
    "# from joblib import dump\n",
    "\n",
    "# dump(clf, \"XGBoost/new_formula/activation.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classifier model\n",
    "\n",
    "from joblib import load\n",
    "\n",
    "clf = load(\"XGBoost/new_formula/activation.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_05</th>\n",
       "      <th>customer_digital_activity_04</th>\n",
       "      <th>customer_spend_01</th>\n",
       "      <th>customer_industry_spend_01</th>\n",
       "      <th>customer_industry_spend_02</th>\n",
       "      <th>customer_industry_spend_03</th>\n",
       "      <th>customer_industry_spend_04</th>\n",
       "      <th>customer_industry_spend_05</th>\n",
       "      <th>customer_spend_02</th>\n",
       "      <th>customer_spend_03</th>\n",
       "      <th>...</th>\n",
       "      <th>merchant_profile_02</th>\n",
       "      <th>merchant_spend_09</th>\n",
       "      <th>merchant_profile_03</th>\n",
       "      <th>customer_digital_activity_01</th>\n",
       "      <th>merchant_spend_10</th>\n",
       "      <th>customer_profile_03</th>\n",
       "      <th>customer_digital_activity_02</th>\n",
       "      <th>customer_profile_04</th>\n",
       "      <th>customer</th>\n",
       "      <th>merchant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.621171</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112.3340</td>\n",
       "      <td>80.5525</td>\n",
       "      <td>9.0</td>\n",
       "      <td>966.63</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>26299.0</td>\n",
       "      <td>4777.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>33.30</td>\n",
       "      <td>72.268283</td>\n",
       "      <td>7.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>467915</td>\n",
       "      <td>599167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.441944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112.3340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>7122.0</td>\n",
       "      <td>4803.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>793.29</td>\n",
       "      <td>72.268283</td>\n",
       "      <td>7.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>467915</td>\n",
       "      <td>686617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.438082</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112.3340</td>\n",
       "      <td>71.1925</td>\n",
       "      <td>3.0</td>\n",
       "      <td>284.77</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7222.0</td>\n",
       "      <td>14860.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>100.00</td>\n",
       "      <td>72.268283</td>\n",
       "      <td>7.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>467915</td>\n",
       "      <td>829193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.072182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112.3340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>11410.0</td>\n",
       "      <td>11968.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>252.38</td>\n",
       "      <td>72.268283</td>\n",
       "      <td>7.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>467915</td>\n",
       "      <td>1077034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.380853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>302.7925</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1847.0</td>\n",
       "      <td>5842.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>87.50</td>\n",
       "      <td>72.268283</td>\n",
       "      <td>7.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>467915</td>\n",
       "      <td>876647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   distance_05  customer_digital_activity_04  customer_spend_01  \\\n",
       "0     1.621171                           NaN           112.3340   \n",
       "1     2.441944                           NaN           112.3340   \n",
       "2     2.438082                           NaN           112.3340   \n",
       "3     2.072182                           NaN           112.3340   \n",
       "4     2.380853                           NaN           302.7925   \n",
       "\n",
       "   customer_industry_spend_01  customer_industry_spend_02  \\\n",
       "0                     80.5525                         9.0   \n",
       "1                         NaN                         NaN   \n",
       "2                     71.1925                         3.0   \n",
       "3                         NaN                         NaN   \n",
       "4                         NaN                         NaN   \n",
       "\n",
       "   customer_industry_spend_03  customer_industry_spend_04  \\\n",
       "0                      966.63                        12.0   \n",
       "1                         NaN                         NaN   \n",
       "2                      284.77                         4.0   \n",
       "3                         NaN                         NaN   \n",
       "4                         NaN                         NaN   \n",
       "\n",
       "   customer_industry_spend_05  customer_spend_02  customer_spend_03  ...  \\\n",
       "0                        10.0                4.0               41.0  ...   \n",
       "1                         NaN                4.0               41.0  ...   \n",
       "2                         4.0                4.0               41.0  ...   \n",
       "3                         NaN                4.0               41.0  ...   \n",
       "4                         NaN                3.0               37.0  ...   \n",
       "\n",
       "   merchant_profile_02  merchant_spend_09  merchant_profile_03  \\\n",
       "0             0.437500            26299.0               4777.0   \n",
       "1             0.397059             7122.0               4803.0   \n",
       "2                  NaN             7222.0              14860.0   \n",
       "3             0.142857            11410.0              11968.0   \n",
       "4             0.100000             1847.0               5842.0   \n",
       "\n",
       "   customer_digital_activity_01  merchant_spend_10  customer_profile_03  \\\n",
       "0                           0.8              33.30            72.268283   \n",
       "1                           0.8             793.29            72.268283   \n",
       "2                           0.8             100.00            72.268283   \n",
       "3                           0.8             252.38            72.268283   \n",
       "4                           0.8              87.50            72.268283   \n",
       "\n",
       "   customer_digital_activity_02  customer_profile_04  customer  merchant  \n",
       "0                           7.0                423.0    467915    599167  \n",
       "1                           7.0                423.0    467915    686617  \n",
       "2                           7.0                423.0    467915    829193  \n",
       "3                           7.0                423.0    467915   1077034  \n",
       "4                           7.0                423.0    467915    876647  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the eval dataset\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "EVAL_DATA = os.environ.get(\"EVAL_DATA\")\n",
    "\n",
    "eval_df = pd.read_csv(EVAL_DATA)\n",
    "\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only include features used in model training\n",
    "\n",
    "features_eval = filtered_df.columns.to_list()\n",
    "# features_eval = list(\n",
    "#     set().union(filtered_ind_reco_features, filtered_activation_features)\n",
    "# )\n",
    "\n",
    "# assert len(features_eval) == 30\n",
    "\n",
    "# Remove ind_recommended and activation from columns to keep\n",
    "if \"ind_recommended\" in features_eval:\n",
    "    features_eval.remove(\"ind_recommended\")\n",
    "if \"activation\" in features_eval:\n",
    "    features_eval.remove(\"activation\")\n",
    "\n",
    "eval_df = eval_df.loc[:, features_eval]\n",
    "\n",
    "# Impute null values with mean\n",
    "\n",
    "eval_imputed = eval_df.fillna(eval_df.mean())\n",
    "\n",
    "# Verify that none of the remaining features have any NULL/NaN values left after imputation\n",
    "\n",
    "eval_imputed_null_counts = eval_imputed.isnull().sum()\n",
    "\n",
    "# Filter the columns to include only those with null values\n",
    "eval_imputed_null_columns = eval_imputed_null_counts[eval_imputed_null_counts > 0]\n",
    "\n",
    "eval_imputed_null_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Split the eval df into one for ind_reco and one for activation\"\"\"\n",
    "\n",
    "# eval_ind_reco = eval_imputed[\n",
    "#     [\n",
    "#         f\n",
    "#         for f in filtered_ind_reco_features\n",
    "#         if f != \"ind_recommended\" and f != \"activation\"\n",
    "#     ]\n",
    "# ]\n",
    "# eval_activation = eval_imputed[\n",
    "#     [\n",
    "#         f\n",
    "#         for f in filtered_activation_features\n",
    "#         if f != \"ind_recommended\" and f != \"activation\"\n",
    "#     ]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the model on the eval dataset twice, once for ind_recommended = 1 and = 0.\n",
    "\n",
    "# \"\"\"Predict ind_recommended values\"\"\"\n",
    "\n",
    "# X_eval = eval_ind_reco.drop(columns=[\"customer\", \"merchant\"])\n",
    "    \n",
    "# # reorder the columns of X_eval to match that of X\n",
    "# column_order = X_reco.columns\n",
    "# X_eval = X_eval.reindex(columns=column_order)\n",
    "\n",
    "# # Drop the ind_recommended ground truth col that appears after reordering\n",
    "# if \"ind_recommended\" in X_eval:\n",
    "#     X_eval.drop(columns=[\"ind_recommended\"], inplace=True)\n",
    "\n",
    "# # Predict ind_recommended with the clf_reco model\n",
    "# y_reco_eval = clf_reco.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "# y_not_reco_eval = clf_reco.predict_proba(X_eval)[:, 0]\n",
    "\n",
    "# \"\"\"RECOMMENDED\"\"\"\n",
    "# X_eval[\"ind_recommended\"] = 1\n",
    "\n",
    "# column_order = X.columns\n",
    "# X_eval = X_eval.reindex(columns=column_order)\n",
    "\n",
    "# # Predict probabilities\n",
    "# y_proba_eval = clf.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "# # Add the predicted probabilities as a new column to the test dataset\n",
    "# X_eval[\"activation_recommended\"] = y_proba_eval\n",
    "\n",
    "\n",
    "# \"\"\"NOT RECOMMENDED\"\"\"\n",
    "# X_eval[\"ind_recommended\"] = 0\n",
    "\n",
    "# # We reuse the X_eval but ignore the previously computed activation column\n",
    "# X_eval_final = X_eval.drop(columns=[\"activation_recommended\"])\n",
    "\n",
    "# # Predict probabilities\n",
    "# y_proba_eval = clf.predict_proba(X_eval_final)[:, 1]\n",
    "\n",
    "# # Add the predicted probabilities as a new column to the test dataset\n",
    "# X_eval[\"activation_not_recommended\"] = y_proba_eval\n",
    "\n",
    "# ### Add the predicted ind_recommended to the test dataset\n",
    "# X_eval[\"ind_recommended\"] = y_reco_eval\n",
    "# X_eval[\"ind_not_recommended\"] = y_not_reco_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the eval dataset twice, once for ind_recommended = 1 and = 0.\n",
    "\n",
    "\"\"\"Predict ind_recommended values\"\"\"\n",
    "\n",
    "X_eval = eval_imputed.drop(columns=[\"customer\", \"merchant\"])\n",
    "\n",
    "# reorder the columns of X_eval to match that of X\n",
    "column_order = X.columns\n",
    "X_eval = X_eval.reindex(columns=column_order)\n",
    "\n",
    "# Drop the ind_recommended ground truth col that appears after reordering\n",
    "X_eval.drop(columns=[\"ind_recommended\"], inplace=True)\n",
    "\n",
    "# Predict ind_recommended with the clf_reco model\n",
    "y_reco_eval = clf_reco.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "y_not_reco_eval = clf_reco.predict_proba(X_eval)[:, 0]\n",
    "\n",
    "\"\"\"RECOMMENDED\"\"\"\n",
    "X_eval[\"ind_recommended\"] = 1\n",
    "\n",
    "X_eval = X_eval.reindex(columns=column_order)\n",
    "\n",
    "# Predict probabilities\n",
    "y_proba_eval = clf.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "# Add the predicted probabilities as a new column to the test dataset\n",
    "X_eval[\"activation_recommended\"] = y_proba_eval\n",
    "\n",
    "\n",
    "\"\"\"NOT RECOMMENDED\"\"\"\n",
    "X_eval[\"ind_recommended\"] = 0\n",
    "\n",
    "# We reuse the X_eval but ignore the previously computed activation column\n",
    "X_eval_final = X_eval.drop(columns=[\"activation_recommended\"])\n",
    "\n",
    "# Predict probabilities\n",
    "y_proba_eval = clf.predict_proba(X_eval_final)[:, 1]\n",
    "\n",
    "# Add the predicted probabilities as a new column to the test dataset\n",
    "X_eval[\"activation_not_recommended\"] = y_proba_eval\n",
    "\n",
    "### Add the predicted ind_recommended to the test dataset\n",
    "X_eval[\"ind_recommended\"] = y_reco_eval\n",
    "X_eval[\"ind_not_recommended\"] = y_not_reco_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the pred_col to eval\n",
    "\n",
    "eval_imputed[\"predicted_score\"] = (\n",
    "    X_eval[\"activation_recommended\"] * X_eval[\"ind_recommended\"] * 3.01\n",
    "    - X_eval[\"activation_not_recommended\"] * X_eval[\"ind_not_recommended\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the pred_score column on the ORIGINAL DATASET\n",
    "# Eval dataset cannot be used since it is missing the ind_recommended and activation columns\n",
    "# We will mimic what happens to the eval dataset, except on the original dataset.\n",
    "\n",
    "\n",
    "def generate_pred_score(X_sampled, y_sampled, X_reco_sampled, y_reco_sampled):\n",
    "    \"\"\"X_sampled is the same as X above\n",
    "    It contains all features except customer, merchant, activation. It contains ind_recommended.\n",
    "\n",
    "    We want to:\n",
    "    1. Using purely the features alone, predict probability of ind_recommended being 1\n",
    "    2. Using purely the features alone, predict probability of ind_recommended being 0\n",
    "    3. Using features + ind_recommended = 1, predict probability of activation. This is activation given reco.\n",
    "    4. Using features + ind_recommended = 0, predict probability of activation. This is activation without reco.\n",
    "    5. Calculate the predicted score.\n",
    "\n",
    "    y_sampled contains only the activation column.\n",
    "    \"\"\"\n",
    "    X_copy, y_copy, X_reco_copy, y_reco_copy = (\n",
    "        X_sampled.copy(),\n",
    "        y_sampled.copy(),\n",
    "        X_reco_sampled.copy(),\n",
    "        y_reco_sampled.copy(),\n",
    "    )  # prevent mutation of original dataframes\n",
    "\n",
    "    # Predict the probability of ind_recommended being 1\n",
    "    y_predicted_ind_reco_one = clf_reco.predict_proba(X_reco_copy)[:, 1]\n",
    "\n",
    "    # Predict the probability of ind_recommended being 0\n",
    "    y_predicted_ind_reco_zero = clf_reco.predict_proba(X_reco_copy)[:, 0]\n",
    "\n",
    "    # Predict the probability of activation given ind_recommended = 1\n",
    "    X_copy[\"ind_recommended\"] = 1\n",
    "    y_predicted_activation_reco = clf.predict_proba(X_copy)[:, 1]\n",
    "\n",
    "    # Predict the probability of activation given ind_recommended = 0\n",
    "    X_copy[\"ind_recommended\"] = 0\n",
    "    y_predicted_activation_no_reco = clf.predict_proba(X_copy)[:, 1]\n",
    "\n",
    "    # Restore the original ind_recommended column\n",
    "    X_copy[\"ind_recommended\"] = y_reco_copy.values\n",
    "\n",
    "    # Add the predicted probabilities as a new column to the test dataset\n",
    "    X_copy[\"predicted_score\"] = (\n",
    "        y_predicted_ind_reco_one * y_predicted_activation_reco\n",
    "        - y_predicted_ind_reco_zero * y_predicted_activation_no_reco\n",
    "    )\n",
    "\n",
    "    # Add the ground truth activation column back to X_sampled\n",
    "    X_copy[\"activation\"] = y_copy.values\n",
    "\n",
    "    return X_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate the pred_score column on the ORIGINAL DATASET\n",
    "# # Eval dataset cannot be used since it is missing the ind_recommended and activation columns\n",
    "# # We will mimic what happens to the eval dataset, except on the original dataset.\n",
    "\n",
    "\n",
    "# def generate_pred_score(X_sampled, y_sampled):\n",
    "#     \"\"\"X_sampled is the same as X above\n",
    "#     It contains all features except customer, merchant, activation. It contains ind_recommended.\n",
    "\n",
    "#     We want to:\n",
    "#     1. Using purely the features alone, predict probability of ind_recommended being 1\n",
    "#     2. Using purely the features alone, predict probability of ind_recommended being 0\n",
    "#     3. Using features + ind_recommended = 1, predict probability of activation. This is activation given reco.\n",
    "#     4. Using features + ind_recommended = 0, predict probability of activation. This is activation without reco.\n",
    "#     5. Calculate the predicted score.\n",
    "\n",
    "#     y_sampled contains only the activation column.\n",
    "#     \"\"\"\n",
    "#     X_copy, y_copy = (\n",
    "#         X_sampled.copy(),\n",
    "#         y_sampled.copy(),\n",
    "#     )  # prevent mutation of original dataframes\n",
    "\n",
    "#     # # Predict the probability of ind_recommended being 1\n",
    "#     X_copy_without_ind_reco = X_copy.drop(columns=[\"ind_recommended\"])  # raw features\n",
    "#     X_actual_ind_reco = X_copy[\"ind_recommended\"].copy()\n",
    "#     y_predicted_ind_reco_one = clf_reco.predict_proba(X_copy_without_ind_reco)[:, 1]\n",
    "\n",
    "#     # Predict the probability of ind_recommended being 0\n",
    "#     y_predicted_ind_reco_zero = clf_reco.predict_proba(X_copy_without_ind_reco)[:, 0]\n",
    "\n",
    "#     # Predict the probability of activation given ind_recommended = 1\n",
    "#     X_copy[\"ind_recommended\"] = 1\n",
    "#     y_predicted_activation_reco = clf.predict_proba(X_copy)[:, 1]\n",
    "\n",
    "#     # Predict the probability of activation given ind_recommended = 0\n",
    "#     X_copy[\"ind_recommended\"] = 0\n",
    "#     y_predicted_activation_no_reco = clf.predict_proba(X_copy)[:, 1]\n",
    "\n",
    "#     # Restore the original ind_recommended column\n",
    "#     X_copy[\"ind_recommended\"] = X_actual_ind_reco.values\n",
    "\n",
    "#     # Add the predicted probabilities as a new column to the test dataset\n",
    "#     X_copy[\"predicted_score\"] = (\n",
    "#         y_predicted_ind_reco_one * y_predicted_activation_reco\n",
    "#         - y_predicted_ind_reco_zero * y_predicted_activation_no_reco\n",
    "#     )\n",
    "\n",
    "#     # Add the ground truth activation column back to X_sampled\n",
    "#     X_copy[\"activation\"] = y_copy.values\n",
    "\n",
    "#     return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ind_recommended  avg_30d_act\n",
      "0                0     0.000489\n",
      "1                1     0.001984\n"
     ]
    }
   ],
   "source": [
    "# Test the scoring function provided\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scoring_code import incr_act_top10\n",
    "\n",
    "with open(\"scores.txt\", \"w+\") as f:\n",
    "    for i in range(1):\n",
    "        X_sampled_test = generate_pred_score(X, y, X_reco, y_reco)\n",
    "        \n",
    "        # Add the customer and merchant columns back on to the dataset\n",
    "        X_sampled_test[\"customer\"] = df_imputed[\"customer\"].values\n",
    "        X_sampled_test[\"merchant\"] = df_imputed[\"merchant\"].values\n",
    "\n",
    "        score = incr_act_top10(input_df=X_sampled_test, pred_col=\"predicted_score\")\n",
    "\n",
    "        f.write(str(score) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation_recommended</th>\n",
       "      <th>activation_not_recommended</th>\n",
       "      <th>ind_recommended</th>\n",
       "      <th>ind_not_recommended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>0.134526</td>\n",
       "      <td>0.865474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.139324</td>\n",
       "      <td>0.860676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.860970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.203926</td>\n",
       "      <td>0.796074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.229607</td>\n",
       "      <td>0.770393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12604595</th>\n",
       "      <td>0.003960</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.048675</td>\n",
       "      <td>0.951325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12604596</th>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.443891</td>\n",
       "      <td>0.556109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12604597</th>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.002453</td>\n",
       "      <td>0.157051</td>\n",
       "      <td>0.842949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12604598</th>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.308531</td>\n",
       "      <td>0.691469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12604599</th>\n",
       "      <td>0.009933</td>\n",
       "      <td>0.010220</td>\n",
       "      <td>0.322888</td>\n",
       "      <td>0.677112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12604600 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          activation_recommended  activation_not_recommended  ind_recommended  \\\n",
       "0                       0.001196                    0.001737         0.134526   \n",
       "1                       0.000325                    0.000674         0.139324   \n",
       "2                       0.001401                    0.001928         0.139030   \n",
       "3                       0.002034                    0.003162         0.203926   \n",
       "4                       0.000781                    0.000981         0.229607   \n",
       "...                          ...                         ...              ...   \n",
       "12604595                0.003960                    0.004547         0.048675   \n",
       "12604596                0.000965                    0.000866         0.443891   \n",
       "12604597                0.001875                    0.002453         0.157051   \n",
       "12604598                0.000967                    0.001095         0.308531   \n",
       "12604599                0.009933                    0.010220         0.322888   \n",
       "\n",
       "          ind_not_recommended  \n",
       "0                    0.865474  \n",
       "1                    0.860676  \n",
       "2                    0.860970  \n",
       "3                    0.796074  \n",
       "4                    0.770393  \n",
       "...                       ...  \n",
       "12604595             0.951325  \n",
       "12604596             0.556109  \n",
       "12604597             0.842949  \n",
       "12604598             0.691469  \n",
       "12604599             0.677112  \n",
       "\n",
       "[12604600 rows x 4 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = [\"activation_recommended\", \"activation_not_recommended\", \"ind_recommended\", \"ind_not_recommended\"]\n",
    "\n",
    "X_eval[test_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate csv for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the submission columns\n",
    "\n",
    "submission_columns = [\"customer\", \"merchant\", \"predicted_score\"]\n",
    "submission_df = eval_imputed[submission_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer</th>\n",
       "      <th>merchant</th>\n",
       "      <th>predicted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>467915</td>\n",
       "      <td>599167</td>\n",
       "      <td>-0.001019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>467915</td>\n",
       "      <td>686617</td>\n",
       "      <td>-0.000444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>467915</td>\n",
       "      <td>829193</td>\n",
       "      <td>-0.001074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>467915</td>\n",
       "      <td>1077034</td>\n",
       "      <td>-0.001269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>467915</td>\n",
       "      <td>876647</td>\n",
       "      <td>-0.000216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer  merchant  predicted_score\n",
       "0    467915    599167        -0.001019\n",
       "1    467915    686617        -0.000444\n",
       "2    467915    829193        -0.001074\n",
       "3    467915   1077034        -0.001269\n",
       "4    467915    876647        -0.000216"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12604600"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Count the number of current submissions so far\"\"\"\n",
    "\n",
    "SUBMISSIONS_DIR = \"submissions/\"\n",
    "\n",
    "submissions = os.listdir(SUBMISSIONS_DIR)\n",
    "\n",
    "submissions_count = len(submissions)\n",
    "\n",
    "submissions_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(f\"submissions/submission_{submissions_count + 1}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submissions\n",
    "18. XGB, wayne features, emme formula\n",
    "19. RF, wayne features, emme formula\n",
    "20. Gaussian NB, wayne features, emme formula\n",
    "21. Gaussian NB, wayne features, wayne formula\n",
    "22. XGB, separate features for ind_reco and activation, emme formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal value of alpha\n",
    "1. Submission 25: 0.001162, alpha = 3\n",
    "2. Submission 26: 0.0003243, alpha = 10\n",
    "3. Submission 27: 0.0007661, alpha = 6\n",
    "4. Submission 28: 0.0009583, alpha = 4\n",
    "5. Submission 29: 0.00106, alpha = 3.5\n",
    "6. Submission 30: 0.0011244, alpha = 3.25\n",
    "7. Submission 31: 0.0011528, alpha = 3.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjectAlpha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
